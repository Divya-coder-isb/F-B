# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bdIdHqpK-2yu8lLUDgxoZn4p_Rkf5vuW
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, accuracy_score, recall_score, precision_score, f1_score
import joblib
import requests
from io import BytesIO

# Function to download model or vectorizer from GitHub
def load_model(url):
    response = requests.get(url)
    response.raise_for_status()  # Check that the request was successful
    return joblib.load(BytesIO(response.content))

# Load your model and vectorizer
model_url = 'https://github.com/Divya-coder-isb/F-B/blob/main/best_xgboost_model.joblib?raw=true'
vectorizer_url = 'https://github.com/Divya-coder-isb/F-B/blob/main/tfidf_vectorizer.joblib?raw=true'
model = load_model(model_url)
vectorizer = load_model(vectorizer_url)

# Streamlit User Interface
st.title('Model Fairness and Bias Evaluation Dashboard')

# Sidebar for user inputs
threshold = st.sidebar.slider('Classification Threshold', 0.0, 1.0, 0.237, 0.01)

# Sample input for text classification
user_input = st.text_area("Enter text for toxicity prediction:")

# Preprocess and predict
if st.button('Predict'):
    transformed_input = vectorizer.transform([user_input])
    proba = model.predict_proba(transformed_input)[0, 1]
    prediction = (proba >= threshold).astype(int)
    st.write('Probability of Toxicity:', proba)
    st.write('Toxic' if prediction else 'Not Toxic')

    # Metrics Display (assuming binary classification for simplicity)
    st.write("### Model Performance Metrics")
    st.write(f"Accuracy: {accuracy_score:.2f}")
    st.write(f"Recall: {recall_score:.2f}")
    st.write(f"Precision: {precision_score:.2f}")
    st.write(f"F1 Score: {f1_score:.2f}")

# Calculate and visualize bias metrics
def calculate_bias_metrics(predictions, labels, sensitive_attribute):
    # Placeholder for actual bias metric calculations
    # For simplicity, we use made-up data. Replace with your actual computations.
    return {
        "Demographic Parity": np.mean(predictions[labels == 1]) / np.mean(predictions[labels == 0]),
        "Predictive Parity": np.std(predictions),
    }

# Example of a visualization function
def plot_roc_curve(y_true, y_scores):
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    roc_auc = auc(fpr, tpr)
    fig, ax = plt.subplots()
    ax.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.set_title('Receiver Operating Characteristic')
    ax.legend(loc="lower right")
    return fig

# Assuming 'labels' and 'y_scores' can be plotted
# This is a placeholder, replace 'labels' and 'y_scores' with your actual data for ROC plotting
labels = np.random.randint(0, 2, size=100)
y_scores = np.random.rand(100)

roc_plot = plot_roc_curve(labels, y_scores)
st.pyplot(roc_plot)